{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23c89596",
   "metadata": {},
   "source": [
    "# **LABORATORIO 1 - DEEP LEARNING**\n",
    "\n",
    "Integrantes:\n",
    "\n",
    "a) Arturo Magno Barrantes Chuquimia \n",
    "\n",
    "b) Ricardo Amiel Acuña Villogas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6dda30",
   "metadata": {},
   "source": [
    "## **Librerías**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32a069cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b49d944",
   "metadata": {},
   "source": [
    "# **Explorando datasets de train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac3b86ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>SPHSUR</th>\n",
       "      <th>BOABIS</th>\n",
       "      <th>SCIPER</th>\n",
       "      <th>DENNAH</th>\n",
       "      <th>LEPLAT</th>\n",
       "      <th>RHIICT</th>\n",
       "      <th>BOALEP</th>\n",
       "      <th>BOAFAB</th>\n",
       "      <th>PHYCUV</th>\n",
       "      <th>...</th>\n",
       "      <th>SCINAS</th>\n",
       "      <th>LEPNOT</th>\n",
       "      <th>ADEMAR</th>\n",
       "      <th>BOAALM</th>\n",
       "      <th>PHYDIS</th>\n",
       "      <th>RHIORN</th>\n",
       "      <th>LEPFLA</th>\n",
       "      <th>SCIRIZ</th>\n",
       "      <th>DENELE</th>\n",
       "      <th>SCIALT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INCT20955_20190909_050000_0_3.wav</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>INCT20955_20190909_050000_1_4.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INCT20955_20190909_050000_2_5.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INCT20955_20190909_050000_3_6.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>INCT20955_20190909_050000_4_7.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            filename  SPHSUR  BOABIS  SCIPER  DENNAH  LEPLAT  \\\n",
       "0  INCT20955_20190909_050000_0_3.wav       0       0       1       0       0   \n",
       "1  INCT20955_20190909_050000_1_4.wav       1       1       1       0       0   \n",
       "2  INCT20955_20190909_050000_2_5.wav       1       1       1       0       0   \n",
       "3  INCT20955_20190909_050000_3_6.wav       1       1       1       0       0   \n",
       "4  INCT20955_20190909_050000_4_7.wav       1       0       1       0       0   \n",
       "\n",
       "   RHIICT  BOALEP  BOAFAB  PHYCUV  ...  SCINAS  LEPNOT  ADEMAR  BOAALM  \\\n",
       "0       0       0       0       0  ...       0       0       0       0   \n",
       "1       0       0       0       0  ...       0       0       0       0   \n",
       "2       0       0       0       0  ...       0       0       0       0   \n",
       "3       0       0       0       0  ...       0       0       0       0   \n",
       "4       0       0       0       0  ...       0       0       0       0   \n",
       "\n",
       "   PHYDIS  RHIORN  LEPFLA  SCIRIZ  DENELE  SCIALT  \n",
       "0       0       0       0       0       0       0  \n",
       "1       0       0       0       0       0       0  \n",
       "2       0       0       0       0       0       0  \n",
       "3       0       0       0       0       0       0  \n",
       "4       0       0       0       0       0       0  \n",
       "\n",
       "[5 rows x 43 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audios = pd.read_csv('train-audios.csv')\n",
    "audios.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea824050",
   "metadata": {},
   "source": [
    "## **Dataset: Audios a espectrogramas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58c99819",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, audio_dir, csv_path=None, transform=None):\n",
    "        self.audio_dir = audio_dir\n",
    "        self.transform = transform\n",
    "        self.mel_spec = torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128)\n",
    "\n",
    "        if csv_path:\n",
    "            self.data = pd.read_csv(csv_path)\n",
    "            self.files = self.data.iloc[:, 0].values\n",
    "            self.labels = self.data.iloc[:, 1:].values.astype(np.float32)\n",
    "            self.has_labels = True\n",
    "        else:\n",
    "            self.files = sorted([f for f in os.listdir(audio_dir) if f.endswith(\".wav\")])\n",
    "            self.has_labels = False\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filepath = os.path.join(self.audio_dir, self.files[idx])\n",
    "        waveform, sr = torchaudio.load(filepath)\n",
    "        if sr != 16000:\n",
    "            waveform = torchaudio.transforms.Resample(sr, 16000)(waveform)\n",
    "\n",
    "        spec = self.mel_spec(waveform)\n",
    "        spec = torchaudio.functional.amplitude_to_DB(spec, multiplier=10.0, amin=1e-10, db_multiplier=0)\n",
    "        spec = spec.squeeze(0).unsqueeze(0)  # [128, time]\n",
    "\n",
    "        if self.transform:\n",
    "            spec = self.transform(spec)\n",
    "\n",
    "        if self.has_labels:\n",
    "            label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "            return spec, label\n",
    "        else:\n",
    "            return spec, self.files[idx]  # devolvemos el nombre para identificar luego"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3caa9d7b",
   "metadata": {},
   "source": [
    "## **Dataset: frames/videos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3167b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "class VideoFrameDataset(Dataset):\n",
    "    def __init__(self, csv_path, img_dir, transform=None):\n",
    "        self.data = pd.read_csv(csv_path, delim_whitespace=True)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.label_cols = [col for col in self.data.columns if col not in ['original_vido_id', 'video_id', 'frame_id', 'path', 'type']]\n",
    "        self.labels = self.data[self.label_cols].values.astype(np.float32)\n",
    "        self.paths = self.data['path'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.paths[idx])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = self.labels[idx]\n",
    "        return image, torch.tensor(label, dtype=torch.float32)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f44b8a",
   "metadata": {},
   "source": [
    "### **Modelo: ResNet18**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fba4228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo CNN\n",
    "class MultiLabelCNN(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(MultiLabelCNN, self).__init__()\n",
    "        self.base = models.resnet18(pretrained=True)\n",
    "        self.base.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.base.fc = nn.Sequential(\n",
    "            nn.Linear(self.base.fc.in_features, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, n_classes),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.base(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf5afbc",
   "metadata": {},
   "source": [
    "#### **Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97793c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    return running_loss / len(dataloader)\n",
    "\n",
    "# Evaluación\n",
    "def eval_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Predicción para set sin etiquetas\n",
    "def predict(model, dataloader, device, threshold=0.5):\n",
    "    model.eval()\n",
    "    results = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, filenames in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            preds = (outputs > threshold).float().cpu().numpy()\n",
    "            for name, pred in zip(filenames, preds):\n",
    "                results.append((name, pred.tolist()))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f9659e",
   "metadata": {},
   "source": [
    "#### **Métricas (validación)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e0b8b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_multilabel(model, dataloader, device, threshold=0.5):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            preds = (outputs > threshold).float().cpu().numpy()\n",
    "            labels = labels.cpu().numpy()\n",
    "            all_preds.append(preds)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    y_pred = np.vstack(all_preds)\n",
    "    y_true = np.vstack(all_labels)\n",
    "\n",
    "    precision_macro = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    recall_macro = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    f1_macro = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "\n",
    "    precision_micro = precision_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "    recall_micro = recall_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "    f1_micro = f1_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "\n",
    "    return {\n",
    "        \"precision_macro\": precision_macro,\n",
    "        \"recall_macro\": recall_macro,\n",
    "        \"f1_macro\": f1_macro,\n",
    "        \"precision_micro\": precision_micro,\n",
    "        \"recall_micro\": recall_micro,\n",
    "        \"f1_micro\": f1_micro\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e3769d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Godel\\Documents\\GitHub\\big_data_open_genome_2\\env\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Godel\\Documents\\GitHub\\big_data_open_genome_2\\env\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\Godel\\Documents\\GitHub\\big_data_open_genome_2\\env\\Lib\\site-packages\\torchaudio\\functional\\functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Godel\\Documents\\GitHub\\big_data_open_genome_2\\env\\Lib\\site-packages\\torchaudio\\functional\\functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m#train_dataset = AudioDataset(audio_dir='audios_train/train/', df=train_audios)\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m#val_dataset = AudioDataset(audio_dir='audios_train/train/', df=val_audios)\u001b[39;00m\n\u001b[32m     26\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     29\u001b[39m \n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m#Entrenamiento\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m10\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     loss = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m     \u001b[38;5;66;03m#loss = train_model(model, train_audios, criterion, optimizer, device)\u001b[39;00m\n\u001b[32m     34\u001b[39m     \u001b[38;5;66;03m#val = eval_model(model,val_audios, criterion, optimizer, device)\u001b[39;00m\n\u001b[32m     35\u001b[39m     \u001b[38;5;66;03m#print(f\"Época {epoch+1} - Pérdida Train: {loss:.4f} - Pérdida Validación: {val:.4f}\")\u001b[39;00m\n\u001b[32m     36\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mÉpoca \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - Pérdida Train: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, dataloader, criterion, optimizer, device)\u001b[39m\n\u001b[32m      2\u001b[39m model.train()\n\u001b[32m      3\u001b[39m running_loss = \u001b[32m0\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Godel\\Documents\\GitHub\\big_data_open_genome_2\\env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    714\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Godel\\Documents\\GitHub\\big_data_open_genome_2\\env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    763\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    765\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    766\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Godel\\Documents\\GitHub\\big_data_open_genome_2\\env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mAudioDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[32m     20\u001b[39m     filepath = os.path.join(\u001b[38;5;28mself\u001b[39m.audio_dir, \u001b[38;5;28mself\u001b[39m.files[idx])\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     waveform, sr = \u001b[43mtorchaudio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m sr != \u001b[32m16000\u001b[39m:\n\u001b[32m     23\u001b[39m         waveform = torchaudio.transforms.Resample(sr, \u001b[32m16000\u001b[39m)(waveform)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Godel\\Documents\\GitHub\\big_data_open_genome_2\\env\\Lib\\site-packages\\torchaudio\\_backend\\utils.py:205\u001b[39m, in \u001b[36mget_load_func.<locals>.load\u001b[39m\u001b[34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Load audio data from source.\u001b[39;00m\n\u001b[32m    129\u001b[39m \n\u001b[32m    130\u001b[39m \u001b[33;03mBy default (``normalize=True``, ``channels_first=True``), this function returns Tensor with\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    202\u001b[39m \u001b[33;03m        `[channel, time]` else `[time, channel]`.\u001b[39;00m\n\u001b[32m    203\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    204\u001b[39m backend = dispatcher(uri, \u001b[38;5;28mformat\u001b[39m, backend)\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels_first\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Godel\\Documents\\GitHub\\big_data_open_genome_2\\env\\Lib\\site-packages\\torchaudio\\_backend\\soundfile.py:27\u001b[39m, in \u001b[36mSoundfileBackend.load\u001b[39m\u001b[34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\n\u001b[32m     19\u001b[39m     uri: Union[BinaryIO, \u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m   (...)\u001b[39m\u001b[32m     25\u001b[39m     buffer_size: \u001b[38;5;28mint\u001b[39m = \u001b[32m4096\u001b[39m,\n\u001b[32m     26\u001b[39m ) -> Tuple[torch.Tensor, \u001b[38;5;28mint\u001b[39m]:\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msoundfile_backend\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels_first\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Godel\\Documents\\GitHub\\big_data_open_genome_2\\env\\Lib\\site-packages\\torchaudio\\_backend\\soundfile_backend.py:221\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(filepath, frame_offset, num_frames, normalize, channels_first, format)\u001b[39m\n\u001b[32m    139\u001b[39m \u001b[38;5;129m@_requires_soundfile\u001b[39m\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\n\u001b[32m    141\u001b[39m     filepath: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    146\u001b[39m     \u001b[38;5;28mformat\u001b[39m: Optional[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    147\u001b[39m ) -> Tuple[torch.Tensor, \u001b[38;5;28mint\u001b[39m]:\n\u001b[32m    148\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load audio data from file.\u001b[39;00m\n\u001b[32m    149\u001b[39m \n\u001b[32m    150\u001b[39m \u001b[33;03m    Note:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    219\u001b[39m \u001b[33;03m            `[channel, time]` else `[time, channel]`.\u001b[39;00m\n\u001b[32m    220\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43msoundfile\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSoundFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file_:\n\u001b[32m    222\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m file_.format != \u001b[33m\"\u001b[39m\u001b[33mWAV\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m normalize:\n\u001b[32m    223\u001b[39m             dtype = \u001b[33m\"\u001b[39m\u001b[33mfloat32\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Godel\\Documents\\GitHub\\big_data_open_genome_2\\env\\Lib\\site-packages\\soundfile.py:690\u001b[39m, in \u001b[36mSoundFile.__init__\u001b[39m\u001b[34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd, compression_level, bitrate_mode)\u001b[39m\n\u001b[32m    687\u001b[39m \u001b[38;5;28mself\u001b[39m._bitrate_mode = bitrate_mode\n\u001b[32m    688\u001b[39m \u001b[38;5;28mself\u001b[39m._info = _create_info_struct(file, mode, samplerate, channels,\n\u001b[32m    689\u001b[39m                                  \u001b[38;5;28mformat\u001b[39m, subtype, endian)\n\u001b[32m--> \u001b[39m\u001b[32m690\u001b[39m \u001b[38;5;28mself\u001b[39m._file = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_int\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosefd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    691\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mset\u001b[39m(mode).issuperset(\u001b[33m'\u001b[39m\u001b[33mr+\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.seekable():\n\u001b[32m    692\u001b[39m     \u001b[38;5;66;03m# Move write position to 0 (like in Python file objects)\u001b[39;00m\n\u001b[32m    693\u001b[39m     \u001b[38;5;28mself\u001b[39m.seek(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Godel\\Documents\\GitHub\\big_data_open_genome_2\\env\\Lib\\site-packages\\soundfile.py:1254\u001b[39m, in \u001b[36mSoundFile._open\u001b[39m\u001b[34m(self, file, mode_int, closefd)\u001b[39m\n\u001b[32m   1252\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1253\u001b[39m             file = file.encode(_sys.getfilesystemencoding())\n\u001b[32m-> \u001b[39m\u001b[32m1254\u001b[39m     file_ptr = \u001b[43mopenfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_int\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_info\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1255\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(file, \u001b[38;5;28mint\u001b[39m):\n\u001b[32m   1256\u001b[39m     file_ptr = _snd.sf_open_fd(file, mode_int, \u001b[38;5;28mself\u001b[39m._info, closefd)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "n_batch = 64\n",
    "n_classes = 42\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MultiLabelCNN(n_classes=n_classes).to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss() # Binary Cross Entropy\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Para audio o video (elige el dataset que corresponda)\n",
    "# dataset = AudioDataset(...) o VideoFrameDataset(...)\n",
    "#### csv=path='train-audios.csv'\n",
    "dataset = AudioDataset(csv_path='train-audios.csv',audio_dir='audios_train/train/')\n",
    "testset = AudioDataset(audio_dir='audios_test/test/')\n",
    "#dataset = VideoFrameDataset(csv_path='train-videos.csv',video_dir='videos_train/video/')\n",
    "\n",
    "# dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "dataloader = DataLoader(dataset, batch_size = n_batch, shuffle=True)\n",
    "testloader = DataLoader(testset, batch_size = n_batch, shuffle=False)\n",
    "\n",
    "# Validación\n",
    "train_audios, val_audios = train_test_split(audios, test_size=0.2, random_state=42)\n",
    "\n",
    "#train_dataset = AudioDataset(audio_dir='audios_train/train/', df=train_audios)\n",
    "#val_dataset = AudioDataset(audio_dir='audios_train/train/', df=val_audios)\n",
    "\n",
    "#trainloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "#valloader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "#Entrenamiento\n",
    "for epoch in range(5):\n",
    "    loss = train_model(model, dataloader, criterion, optimizer, device)\n",
    "    #loss = train_model(model, train_audios, criterion, optimizer, device)\n",
    "    #val = eval_model(model,val_audios, criterion, optimizer, device)\n",
    "    #print(f\"Época {epoch+1} - Pérdida Train: {loss:.4f} - Pérdida Validación: {val:.4f}\")\n",
    "    print(f\"Época {epoch+1} - Pérdida Train: {loss:.4f}\")\n",
    "    \n",
    "#val_metrics = evaluate_multilabel(model, val_audios, device)\n",
    "#print(f\"[Validación] Precision_macro: {val_metrics['precision_macro']:.3f}, F1_micro: {val_metrics['f1_micro']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c496d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inferencia\n",
    "results = predict(model, testloader, device)\n",
    "# Guardar a CSV\n",
    "df_results = pd.DataFrame([\n",
    "    [filename] + preds for filename, preds in results\n",
    "])\n",
    "columns = ['filename'] + list(dataset.data.columns[1:])  # mismo orden de etiquetas\n",
    "df_results.columns = columns\n",
    "df_results.to_csv(\"audios_predicciones_test.csv\", index=False)\n",
    "\n",
    "# Mostrar los 5 primeros resultados\n",
    "for name, preds in results[:5]:\n",
    "    print(f\"{name} → {preds}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
